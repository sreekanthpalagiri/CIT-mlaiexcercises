{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1 Excercise for Applied ML, MS in AI - Basics of Numpy and Pandas\n",
    "\n",
    "First step - Set current director to the course directory and print the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Work\\CIT\\COMP9061-ML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os. chdir(\"C:\\Work\\CIT\\COMP9061-ML\")\n",
    "dir = os.getcwd()\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Numerical Analysis Exercises using NumPy – Rainfall Dataset:\n",
    "\n",
    "The objective of the exercises below is to familiarize yourself with the use of NumPy. These excises are mainly based on rainfall data in Cork for each month over the past half century. In the folder you will find a file called CorkRainfall.txt and a file called DublinRainfall.txt. This is a space delimited file.\n",
    "Each line of the file contains the following precipitation information pertaining to a specific month and year:\n",
    "\n",
    "• Year\n",
    "• Month (1 = Jan, 2 = Feb, 3 = March, etc. )\n",
    "• Total Rainfall (Millimetres)\n",
    "• Most Rainfall in a Day (Millimetres)\n",
    "• Number of Rain days (A day is classified as a rain day if it has >= 0.2mm rain) (Number)\n",
    "\n",
    "Please use NumPy to answer the following questions. The objective of this task is to familiarize yourself with the operation of NumPy (there is no need to incorporate error checking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read csv to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year  Month  total  maxinaday  Norainday\n",
      "0    1962      1  134.6       12.5         22\n",
      "1    1962      2   77.2       11.2         17\n",
      "2    1962      3   73.6       13.4         13\n",
      "3    1962      4   68.6       20.6         17\n",
      "4    1962      5   80.7       22.7         16\n",
      "5    1962      6   50.2       28.3         11\n",
      "6    1962      7   92.2       18.5         15\n",
      "7    1962      8   99.4       25.8         19\n",
      "8    1962      9  122.4       31.1         19\n",
      "9    1962     10   42.6       15.1         12\n",
      "10   1962     11  105.5       23.7         14\n",
      "11   1962     12   86.4       23.6         18\n",
      "12   1963      1   34.8       14.9         13\n",
      "13   1963      2   96.0       19.7         11\n",
      "14   1963      3  212.5       30.5         25\n",
      "15   1963      4  113.7       29.2         20\n",
      "16   1963      5   53.9        7.6         19\n",
      "17   1963      6   46.2        8.6         15\n",
      "18   1963      7   76.6       16.9         14\n",
      "19   1963      8  104.3       40.6         22\n",
      "20   1963      9   39.5        8.8         16\n",
      "21   1963     10  143.1       56.0         21\n",
      "22   1963     11  206.9       35.2         24\n",
      "23   1963     12  112.9       37.0         17\n",
      "24   1964      1   35.9       12.6         17\n",
      "25   1964      2  166.4       46.3         19\n",
      "26   1964      3  233.9       28.7         20\n",
      "27   1964      4   82.9       16.6         20\n",
      "28   1964      5  106.7       17.1         21\n",
      "29   1964      6   89.8       18.1         16\n",
      "..    ...    ...    ...        ...        ...\n",
      "570  2009      7  203.7       36.7         27\n",
      "571  2009      8  155.5       24.1         23\n",
      "572  2009      9   53.8       15.8         11\n",
      "573  2009     10  180.6       27.1         21\n",
      "574  2009     11  232.8       30.3         29\n",
      "575  2009     12  157.6       41.9         19\n",
      "576  2010      1  125.7       21.5         14\n",
      "577  2010      2   42.7       19.5         12\n",
      "578  2010      3   96.2       18.9         17\n",
      "579  2010      4   40.9       18.0         12\n",
      "580  2010      5   47.3       10.5         12\n",
      "581  2010      6   63.4       21.1         15\n",
      "582  2010      7  119.5       25.2         22\n",
      "583  2010      8   17.1        8.8         10\n",
      "584  2010      9   81.4       16.7         22\n",
      "585  2010     10  118.0       26.2         24\n",
      "586  2010     11   85.6       29.2         20\n",
      "587  2010     12   66.9       27.1          9\n",
      "588  2011      1   75.8       17.7         11\n",
      "589  2011      2  127.7       17.0         23\n",
      "590  2011      3   35.7        7.0         12\n",
      "591  2011      4   39.7       12.1         13\n",
      "592  2011      5  122.3       22.4         18\n",
      "593  2011      6   88.7       23.8         17\n",
      "594  2011      7   40.3       13.2         13\n",
      "595  2011      8   53.9        9.3         14\n",
      "596  2011      9  112.5       16.8         24\n",
      "597  2011     10   91.8       24.4         22\n",
      "598  2011     11  148.6       35.2         23\n",
      "599  2011     12   87.1       22.0         26\n",
      "\n",
      "[600 rows x 5 columns]\n",
      "     Year  Month  total  maxinaday  Norainday\n",
      "0    1962      1   41.1        7.1         16\n",
      "1    1962      2   26.7        5.7         14\n",
      "2    1962      3   53.6       15.1         13\n",
      "3    1962      4   40.8        7.1         13\n",
      "4    1962      5   56.8       18.5         19\n",
      "5    1962      6   24.9        7.4         11\n",
      "6    1962      7   68.0       18.4         15\n",
      "7    1962      8   73.1       17.5         17\n",
      "8    1962      9  129.2       31.1         19\n",
      "9    1962     10   16.8        8.4          8\n",
      "10   1962     11   47.9        9.2         14\n",
      "11   1962     12   75.3       12.9         20\n",
      "12   1963      1   41.1        9.3         12\n",
      "13   1963      2   61.9       22.7         10\n",
      "14   1963      3   65.2       15.2         16\n",
      "15   1963      4   47.0        6.6         19\n",
      "16   1963      5   42.0        7.9         20\n",
      "17   1963      6   54.3       15.5         17\n",
      "18   1963      7   23.5        6.0         13\n",
      "19   1963      8  103.1       53.5         20\n",
      "20   1963      9   44.9       12.0         12\n",
      "21   1963     10   47.6        9.7         15\n",
      "22   1963     11   95.9       17.8         21\n",
      "23   1963     12   16.4        4.4         10\n",
      "24   1964      1   14.8        3.4         10\n",
      "25   1964      2   28.3        5.8         13\n",
      "26   1964      3  106.6       18.9         18\n",
      "27   1964      4   62.3       15.0         23\n",
      "28   1964      5   45.4        7.0         16\n",
      "29   1964      6   51.0        9.6         17\n",
      "..    ...    ...    ...        ...        ...\n",
      "570  2009      7  153.9       39.2         22\n",
      "571  2009      8   69.1       22.2         20\n",
      "572  2009      9   24.2       13.0          9\n",
      "573  2009     10   84.9       22.9         20\n",
      "574  2009     11  149.4       28.1         26\n",
      "575  2009     12   71.6       20.4         18\n",
      "576  2010      1   42.7       10.0         14\n",
      "577  2010      2   36.7        8.5         14\n",
      "578  2010      3   54.8       16.0         11\n",
      "579  2010      4   26.7        5.2         11\n",
      "580  2010      5   48.8       19.5         13\n",
      "581  2010      6   43.3       15.5          7\n",
      "582  2010      7   74.7       19.5         18\n",
      "583  2010      8   48.0        8.8         17\n",
      "584  2010      9  105.5       17.7         19\n",
      "585  2010     10   29.6        6.0         12\n",
      "586  2010     11  100.5       11.1         24\n",
      "587  2010     12   57.6       12.0         16\n",
      "588  2011      1   29.4       10.5         13\n",
      "589  2011      2   75.9       18.7         22\n",
      "590  2011      3   19.4        8.1         10\n",
      "591  2011      4   28.4       12.6         12\n",
      "592  2011      5   37.1        6.1         21\n",
      "593  2011      6   64.2       10.2         18\n",
      "594  2011      7   42.6        7.0         17\n",
      "595  2011      8   39.7        6.7         19\n",
      "596  2011      9   87.3       42.1         19\n",
      "597  2011     10  147.4       71.3         18\n",
      "598  2011     11   48.5       11.7         11\n",
      "599  2011     12   52.2        5.2         23\n",
      "\n",
      "[600 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_cork = pd.read_csv(dir+'\\datasets\\CorkRainfall.txt', index_col= False,\n",
    "           names  = ['Year','Month','total','maxinaday','Norainday'], delimiter =' ')\n",
    "df_dublin = pd.read_csv(dir+'\\datasets\\DublinRainfall.txt',index_col= False,\n",
    "            names  = ['Year','Month','total','maxinaday','Norainday'],delimiter =' ')\n",
    "\n",
    "print(df_cork)\n",
    "print(df_dublin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Print out the max ‘Most Rainfall in a Day’ value and the average ‘Most Rainfall in a Day’ value for the Cork data (that is, obtain the maximum value contained in this column of data and the average value in this column of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.7 22.910500000000024\n"
     ]
    }
   ],
   "source": [
    "print(df_cork.maxinaday.max(),df_cork.maxinaday.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Display all unique years for which there is data in the dataset (you can use np.unique) Ask the user to select a specific year. You should then output the sum of the Rain Days column for that year (you do this by adding up the \"Number of rain days” for all 12 rows pertaining to the selected year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick an year\n",
      "[1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975\n",
      " 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989\n",
      " 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003\n",
      " 2004 2005 2006 2007 2008 2009 2010 2011]\n",
      "1962\n",
      "Total No. of rain days: 193\n"
     ]
    }
   ],
   "source": [
    "print('Pick an year')\n",
    "print(df_cork.Year.unique())\n",
    "useryear = int(input())\n",
    "print('Total No. of rain days:',df_cork[df_cork.Year ==useryear].Norainday.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Calculate the wettest month of the year in Cork based using the “Total Rainfall” value. The month that has the highest cumulative “Total Rainfall” value across all years should be classified as the wettest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest rain fall in year 1962 was in month 1, which is 134.6\n",
      "Highest rain fall in year 1963 was in month 3, which is 212.5\n",
      "Highest rain fall in year 1964 was in month 3, which is 233.9\n",
      "Highest rain fall in year 1965 was in month 11, which is 206.9\n",
      "Highest rain fall in year 1966 was in month 2, which is 299.9\n",
      "Highest rain fall in year 1967 was in month 10, which is 151.1\n",
      "Highest rain fall in year 1968 was in month 11, which is 186.8\n",
      "Highest rain fall in year 1969 was in month 1, which is 275.0\n",
      "Highest rain fall in year 1970 was in month 1, which is 219.1\n",
      "Highest rain fall in year 1971 was in month 1, which is 162.6\n",
      "Highest rain fall in year 1972 was in month 2, which is 190.5\n",
      "Highest rain fall in year 1973 was in month 5, which is 182.5\n",
      "Highest rain fall in year 1974 was in month 1, which is 340.9\n",
      "Highest rain fall in year 1975 was in month 10, which is 204.7\n",
      "Highest rain fall in year 1976 was in month 10, which is 209.8\n",
      "Highest rain fall in year 1977 was in month 2, which is 237.1\n",
      "Highest rain fall in year 1978 was in month 12, which is 265.0\n",
      "Highest rain fall in year 1979 was in month 12, which is 216.4\n",
      "Highest rain fall in year 1980 was in month 10, which is 157.5\n",
      "Highest rain fall in year 1981 was in month 5, which is 216.4\n",
      "Highest rain fall in year 1982 was in month 10, which is 209.5\n",
      "Highest rain fall in year 1983 was in month 9, which is 194.4\n",
      "Highest rain fall in year 1984 was in month 1, which is 194.0\n",
      "Highest rain fall in year 1985 was in month 12, which is 180.6\n",
      "Highest rain fall in year 1986 was in month 8, which is 226.9\n",
      "Highest rain fall in year 1987 was in month 12, which is 184.7\n",
      "Highest rain fall in year 1988 was in month 1, which is 256.9\n",
      "Highest rain fall in year 1989 was in month 12, which is 239.0\n",
      "Highest rain fall in year 1990 was in month 2, which is 213.4\n",
      "Highest rain fall in year 1991 was in month 10, which is 148.2\n",
      "Highest rain fall in year 1992 was in month 11, which is 141.0\n",
      "Highest rain fall in year 1993 was in month 9, which is 216.0\n",
      "Highest rain fall in year 1994 was in month 2, which is 236.6\n",
      "Highest rain fall in year 1995 was in month 10, which is 234.3\n",
      "Highest rain fall in year 1996 was in month 10, which is 239.7\n",
      "Highest rain fall in year 1997 was in month 8, which is 231.3\n",
      "Highest rain fall in year 1998 was in month 1, which is 178.5\n",
      "Highest rain fall in year 1999 was in month 8, which is 199.8\n",
      "Highest rain fall in year 2000 was in month 11, which is 190.9\n",
      "Highest rain fall in year 2001 was in month 8, which is 154.6\n",
      "Highest rain fall in year 2002 was in month 1, which is 242.4\n",
      "Highest rain fall in year 2003 was in month 11, which is 125.0\n",
      "Highest rain fall in year 2004 was in month 8, which is 178.5\n",
      "Highest rain fall in year 2005 was in month 10, which is 219.3\n",
      "Highest rain fall in year 2006 was in month 10, which is 184.7\n",
      "Highest rain fall in year 2007 was in month 6, which is 156.9\n",
      "Highest rain fall in year 2008 was in month 1, which is 195.1\n",
      "Highest rain fall in year 2009 was in month 11, which is 232.8\n",
      "Highest rain fall in year 2010 was in month 1, which is 125.7\n",
      "Highest rain fall in year 2011 was in month 11, which is 148.6\n",
      "Wettest Month Across all years: 1\n"
     ]
    }
   ],
   "source": [
    "df_cork_year = df_cork.groupby(['Year'])['total'].max()\n",
    "for index,row in df_cork.iterrows():\n",
    "    if df_cork_year[row['Year']] == row['total']: \n",
    "        print('Highest rain fall in year {0} was in month {1}, which is {2}'.format(int(row['Year']), \n",
    "            int(row['Month']),row['total']))\n",
    "df_cork_month = df_cork.groupby('Month').agg({'total': ['max']})\n",
    "df_cork_month.columns = ['month_max']\n",
    "print('Wettest Month Across all years:',df_cork_month[df_cork_month.month_max==df_cork_month.month_max.max()].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) This question focuses on the Number of Rain days column. The user is asked to enter a maximum threshold value for the number of rain days. Your code should then output the percentage of the time (percentage of rows in the dataset) where the number of rain days is less than or equal to the threshold value.\n",
    "##### Practical Machine Learning –\n",
    "##### NumPy Exercises\n",
    "##### For example, if a user enters a maximum threshold value of 6, then your code should output the percentage of rows where the number of rain days fell below the threshold value of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "28 % times it rained below 15 days in a month\n"
     ]
    }
   ],
   "source": [
    "threshhold=int(input())\n",
    "print('{0} % times it rained below {1} days in a month.'.format((round((df_cork[df_cork.Norainday<threshhold].shape[0]/df_cork.shape[0])*100)),threshhold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### v) Calculate the average ‘total rainfall’ value for the summer months (June, July and August) and the Autumn months (Sept, Oct, Nov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year  Season\n",
      "1962  Autumn    122.4\n",
      "      Summer     99.4\n",
      "1963  Autumn    206.9\n",
      "      Summer    104.3\n",
      "1964  Autumn     80.2\n",
      "      Summer    134.6\n",
      "1965  Autumn    206.9\n",
      "      Summer    128.8\n",
      "1966  Autumn    232.2\n",
      "      Summer    102.2\n",
      "1967  Autumn    151.1\n",
      "      Summer    101.9\n",
      "1968  Autumn    186.8\n",
      "      Summer     74.6\n",
      "1969  Autumn    113.1\n",
      "      Summer     84.5\n",
      "1970  Autumn    216.3\n",
      "      Summer     86.3\n",
      "1971  Autumn    128.5\n",
      "      Summer    121.8\n",
      "1972  Autumn    121.9\n",
      "      Summer     44.2\n",
      "1973  Autumn    156.3\n",
      "      Summer     74.6\n",
      "1974  Autumn    187.4\n",
      "      Summer    118.9\n",
      "1975  Autumn    204.7\n",
      "      Summer    186.3\n",
      "1976  Autumn    209.8\n",
      "      Summer     92.8\n",
      "                ...  \n",
      "1997  Autumn    215.6\n",
      "      Summer    231.3\n",
      "1998  Autumn    167.1\n",
      "      Summer    128.9\n",
      "1999  Autumn    180.0\n",
      "      Summer    199.8\n",
      "2000  Autumn    190.9\n",
      "      Summer     68.2\n",
      "2001  Autumn    127.2\n",
      "      Summer    154.6\n",
      "2002  Autumn    214.7\n",
      "      Summer    105.3\n",
      "2003  Autumn    125.0\n",
      "      Summer    107.0\n",
      "2004  Autumn    167.1\n",
      "      Summer    178.5\n",
      "2005  Autumn    219.3\n",
      "      Summer    105.2\n",
      "2006  Autumn    184.7\n",
      "      Summer     69.9\n",
      "2007  Autumn     64.0\n",
      "      Summer    156.9\n",
      "2008  Autumn    139.2\n",
      "      Summer    163.6\n",
      "2009  Autumn    232.8\n",
      "      Summer    203.7\n",
      "2010  Autumn    118.0\n",
      "      Summer    119.5\n",
      "2011  Autumn    148.6\n",
      "      Summer     88.7\n",
      "Name: total, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_cork1= df_cork\n",
    "df_cork1['Season']=''\n",
    "df_cork1.loc[df_cork1.Month.isin([6,7,8]),'Season']='Summer'\n",
    "df_cork1.loc[df_cork1.Month.isin([9,10,11]),'Season']='Autumn'\n",
    "df_cork_specmon=df_cork1[df_cork1.Season!='']\n",
    "df_cork_year_season = df_cork_specmon.groupby(['Year','Season'])['total'].max()\n",
    "print(df_cork_year_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Read in the contents of the file DublinRainfall.txt into a NumPy array. Append the all rows from the Dublin array to the Cork NumPy array. Calculate the average number of raindays for the new array and write the new NumPy array to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.701666666666668\n"
     ]
    }
   ],
   "source": [
    "df_rain = pd.concat([df_cork, df_dublin],sort=False).reset_index()\n",
    "print(df_rain.Norainday.mean())\n",
    "df_rain.to_csv(dir+'\\datasets\\FullRainfall.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Numerical Analysis Exercises using NumPy Bike Dataset:\n",
    "For each of the following questions you will use the bike rental dataset called bike.csv.\n",
    "Where possible use NumPy to answer the questions below.\n",
    "The following are the details of the various fields in this dataset.\n",
    "0. instant: record index\n",
    "1. season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "2. yr : year (0: 2011, 1:2012)\n",
    "3. mnth : month ( 1 to 12)\n",
    "4. hr : hour (0 to 23)\n",
    "5. holiday : weather day is holiday or not (extracted from [Web Link])\n",
    "6. weekday : day of the week\n",
    "7. workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "8. + weathersit :\n",
    "i. 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "ii. 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "iii. 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "iv. 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "9. temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "10. atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "11. hum: Normalized humidity. The values are divided to 100 (max)\n",
    "12. windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "13. casual: count of casual users\n",
    "14. registered: count of registered users\n",
    "15. cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       season  yr  mnth  hr  holiday  weekday  workingday  weathersit  temp  \\\n",
      "1           1   0     1   0        0        6           0           1  0.24   \n",
      "2           1   0     1   1        0        6           0           1  0.22   \n",
      "3           1   0     1   2        0        6           0           1  0.22   \n",
      "4           1   0     1   3        0        6           0           1  0.24   \n",
      "5           1   0     1   4        0        6           0           1  0.24   \n",
      "6           1   0     1   5        0        6           0           2  0.24   \n",
      "7           1   0     1   6        0        6           0           1  0.22   \n",
      "8           1   0     1   7        0        6           0           1  0.20   \n",
      "9           1   0     1   8        0        6           0           1  0.24   \n",
      "10          1   0     1   9        0        6           0           1  0.32   \n",
      "11          1   0     1  10        0        6           0           1  0.38   \n",
      "12          1   0     1  11        0        6           0           1  0.36   \n",
      "13          1   0     1  12        0        6           0           1  0.42   \n",
      "14          1   0     1  13        0        6           0           2  0.46   \n",
      "15          1   0     1  14        0        6           0           2  0.46   \n",
      "16          1   0     1  15        0        6           0           2  0.44   \n",
      "17          1   0     1  16        0        6           0           2  0.42   \n",
      "18          1   0     1  17        0        6           0           2  0.44   \n",
      "19          1   0     1  18        0        6           0           3  0.42   \n",
      "20          1   0     1  19        0        6           0           3  0.42   \n",
      "21          1   0     1  20        0        6           0           2  0.40   \n",
      "22          1   0     1  21        0        6           0           2  0.40   \n",
      "23          1   0     1  22        0        6           0           2  0.40   \n",
      "24          1   0     1  23        0        6           0           2  0.46   \n",
      "25          1   0     1   0        0        0           0           2  0.46   \n",
      "26          1   0     1   1        0        0           0           2  0.44   \n",
      "27          1   0     1   2        0        0           0           2  0.42   \n",
      "28          1   0     1   3        0        0           0           2  0.46   \n",
      "29          1   0     1   4        0        0           0           2  0.46   \n",
      "30          1   0     1   6        0        0           0           3  0.42   \n",
      "...       ...  ..   ...  ..      ...      ...         ...         ...   ...   \n",
      "17350       1   1    12  18        0        0           0           2  0.24   \n",
      "17351       1   1    12  19        0        0           0           1  0.34   \n",
      "17352       1   1    12  20        0        0           0           1  0.22   \n",
      "17353       1   1    12  21        0        0           0           1  0.20   \n",
      "17354       1   1    12  22        0        0           0           1  0.20   \n",
      "17355       1   1    12  23        0        0           0           1  0.20   \n",
      "17356       1   1    12   0        0        1           1           1  0.18   \n",
      "17357       1   1    12   1        0        1           1           1  0.18   \n",
      "17358       1   1    12   2        0        1           1           1  0.16   \n",
      "17359       1   1    12   3        0        1           1           1  0.16   \n",
      "17360       1   1    12   4        0        1           1           1  0.14   \n",
      "17361       1   1    12   5        0        1           1           1  0.16   \n",
      "17362       1   1    12   6        0        1           1           1  0.16   \n",
      "17363       1   1    12   7        0        1           1           1  0.16   \n",
      "17364       1   1    12   8        0        1           1           1  0.14   \n",
      "17365       1   1    12   9        0        1           1           2  0.18   \n",
      "17366       1   1    12  10        0        1           1           2  0.20   \n",
      "17367       1   1    12  11        0        1           1           2  0.22   \n",
      "17368       1   1    12  12        0        1           1           2  0.24   \n",
      "17369       1   1    12  13        0        1           1           2  0.26   \n",
      "17370       1   1    12  14        0        1           1           2  0.28   \n",
      "17371       1   1    12  15        0        1           1           2  0.28   \n",
      "17372       1   1    12  16        0        1           1           2  0.26   \n",
      "17373       1   1    12  17        0        1           1           2  0.26   \n",
      "17374       1   1    12  18        0        1           1           2  0.26   \n",
      "17375       1   1    12  19        0        1           1           2  0.26   \n",
      "17376       1   1    12  20        0        1           1           2  0.26   \n",
      "17377       1   1    12  21        0        1           1           1  0.26   \n",
      "17378       1   1    12  22        0        1           1           1  0.26   \n",
      "17379       1   1    12  23        0        1           1           1  0.26   \n",
      "\n",
      "        atemp   hum  windspeed  casual  registered  cnt  \n",
      "1      0.2879  0.81     0.0000       3          13   16  \n",
      "2      0.2727  0.80     0.0000       8          32   40  \n",
      "3      0.2727  0.80     0.0000       5          27   32  \n",
      "4      0.2879  0.75     0.0000       3          10   13  \n",
      "5      0.2879  0.75     0.0000       0           1    1  \n",
      "6      0.2576  0.75     0.0896       0           1    1  \n",
      "7      0.2727  0.80     0.0000       2           0    2  \n",
      "8      0.2576  0.86     0.0000       1           2    3  \n",
      "9      0.2879  0.75     0.0000       1           7    8  \n",
      "10     0.3485  0.76     0.0000       8           6   14  \n",
      "11     0.3939  0.76     0.2537      12          24   36  \n",
      "12     0.3333  0.81     0.2836      26          30   56  \n",
      "13     0.4242  0.77     0.2836      29          55   84  \n",
      "14     0.4545  0.72     0.2985      47          47   94  \n",
      "15     0.4545  0.72     0.2836      35          71  106  \n",
      "16     0.4394  0.77     0.2985      40          70  110  \n",
      "17     0.4242  0.82     0.2985      41          52   93  \n",
      "18     0.4394  0.82     0.2836      15          52   67  \n",
      "19     0.4242  0.88     0.2537       9          26   35  \n",
      "20     0.4242  0.88     0.2537       6          31   37  \n",
      "21     0.4091  0.87     0.2537      11          25   36  \n",
      "22     0.4091  0.87     0.1940       3          31   34  \n",
      "23     0.4091  0.94     0.2239      11          17   28  \n",
      "24     0.4545  0.88     0.2985      15          24   39  \n",
      "25     0.4545  0.88     0.2985       4          13   17  \n",
      "26     0.4394  0.94     0.2537       1          16   17  \n",
      "27     0.4242  1.00     0.2836       1           8    9  \n",
      "28     0.4545  0.94     0.1940       2           4    6  \n",
      "29     0.4545  0.94     0.1940       2           1    3  \n",
      "30     0.4242  0.77     0.2985       0           2    2  \n",
      "...       ...   ...        ...     ...         ...  ...  \n",
      "17350  0.2121  0.44     0.2985      12         113  125  \n",
      "17351  0.3636  0.61     0.0000      16          86  102  \n",
      "17352  0.1970  0.47     0.3284       9          63   72  \n",
      "17353  0.2121  0.51     0.1642       5          42   47  \n",
      "17354  0.1970  0.55     0.1940       6          30   36  \n",
      "17355  0.1970  0.51     0.2239      10          39   49  \n",
      "17356  0.1818  0.55     0.1940       4          30   34  \n",
      "17357  0.1818  0.55     0.1940       6          13   19  \n",
      "17358  0.1667  0.59     0.1642       3           8   11  \n",
      "17359  0.1818  0.59     0.1045       0           1    1  \n",
      "17360  0.1667  0.69     0.1045       0           3    3  \n",
      "17361  0.1515  0.64     0.1940       0           9    9  \n",
      "17362  0.1667  0.64     0.1642       0          40   40  \n",
      "17363  0.1818  0.64     0.1343       2          83   85  \n",
      "17364  0.1515  0.69     0.1343       9         187  196  \n",
      "17365  0.2121  0.64     0.1045      13         144  157  \n",
      "17366  0.2121  0.69     0.1343      33          87  120  \n",
      "17367  0.2273  0.60     0.1940      43         114  157  \n",
      "17368  0.2273  0.56     0.1940      52         172  224  \n",
      "17369  0.2576  0.44     0.1642      38         165  203  \n",
      "17370  0.2727  0.45     0.2239      62         185  247  \n",
      "17371  0.2879  0.45     0.1343      69         246  315  \n",
      "17372  0.2576  0.48     0.1940      30         184  214  \n",
      "17373  0.2879  0.48     0.0896      14         150  164  \n",
      "17374  0.2727  0.48     0.1343      10         112  122  \n",
      "17375  0.2576  0.60     0.1642      11         108  119  \n",
      "17376  0.2576  0.60     0.1642       8          81   89  \n",
      "17377  0.2576  0.60     0.1642       7          83   90  \n",
      "17378  0.2727  0.56     0.1343      13          48   61  \n",
      "17379  0.2727  0.65     0.1343      12          37   49  \n",
      "\n",
      "[17379 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "df_bike = pd.read_csv(dir+'\\datasets\\\\bike.csv', \n",
    "           names  = ['season','yr','mnth','hr','holiday','weekday','workingday','weathersit','temp','atemp','hum','windspeed','casual','registered','cnt'])\n",
    "print(df_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Calculate the average temperature value (column index 9) for the entire dataset. Note the temperature values in this column have been already normalized by dividing by 41."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Temperature value is 20.38.\n"
     ]
    }
   ],
   "source": [
    "print('Average Temperature value is {}.'.format(round(df_bike.temp.mean()*41,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Print out the average number of casual users for all days classified as holidays as well as the average for all days classified as non-holidays. (Note holidays =1 and non-holidays = 0). Holidays attribute is stored at index 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average casual users on holidate are 44.72 and on non-holiday are 35.41.\n"
     ]
    }
   ],
   "source": [
    "print('Average casual users on holidate are {0} and on non-holiday are {1}.'.format(round(df_bike[df_bike.holiday==1].casual.mean(),2),round(df_bike[df_bike.holiday==0].casual.mean(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Write NumPy code that will print out the total number of casual users for each month of the year. You would expect to see an increase in the number of casual users over the summer months and a decline for the winter months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yr  mnth\n",
      "0   1        38189\n",
      "    2        48215\n",
      "    3        64045\n",
      "    4        94870\n",
      "    5       135821\n",
      "    6       143512\n",
      "    7       141341\n",
      "    8       136691\n",
      "    9       127418\n",
      "    10      123511\n",
      "    11      102167\n",
      "    12       87323\n",
      "1   1        96744\n",
      "    2       103137\n",
      "    3       164875\n",
      "    4       174224\n",
      "    5       195865\n",
      "    6       202830\n",
      "    7       203607\n",
      "    8       214503\n",
      "    9       218573\n",
      "    10      198841\n",
      "    11      152664\n",
      "    12      123713\n",
      "Name: cnt, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bike_season = df_bike.groupby(['yr','mnth'])['casual'].sum()\n",
    "print(df_bike_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) We will now look at the relationship between temperature and the number of users (column index 15). Your code should work out the average number of users for the following temperature ranges.\n",
    "        • 1, 6\n",
    "        • 6, 10\n",
    "        • 10, 15\n",
    "        • 15, 20\n",
    "        • 20, 25\n",
    "        • 25, 30\n",
    "        • 30, 35\n",
    "        • 35, 40\n",
    "Remember the temperature values specified in the file have been normalised by dividing by 41."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n",
      "(0, 5]        9388\n",
      "(5, 10]     131513\n",
      "(10, 15]    424096\n",
      "(15, 20]    486459\n",
      "(20, 25]    673569\n",
      "Name: cnt, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bike_temp=df_bike.groupby(pd.cut((df_bike.temp*41),np.arange(0, 42, 5)))['cnt'].sum()\n",
    "print(df_bike_temp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Numerical Analysis Exercises Pandas – Shark Attack Dataset:\n",
    "For each of the following questions you will use a dataset containing information on global shark attacks called attacks.csv.\n",
    "Attribute Information:\n",
    "The attributes recorded in the dataset are as follows:\n",
    "0. Case Number\n",
    "1. Date\n",
    "2. Year\n",
    "3. Type\n",
    "4. Country\n",
    "5. Area\n",
    "6. Location\n",
    "7. Activity\n",
    "8. Name\n",
    "9. Sex\n",
    "10. Age\n",
    "11. Injury\n",
    "12. Fatal\n",
    "13. Time\n",
    "14. Species\n",
    "15. Investigator or Source\n",
    "You will notice in the dataset that some entries in the fatality column are recorded as UNKNOWN, n, F, etc. We ignore these entries and only consider entries that are uppercase ‘Y’ or ‘N’.\n",
    "Open this file using Pandas read_csv() function. The data file is stored in a different encoding format so you can use the following line to read the data into a dataframe.\n",
    "df = pd.read_csv('attacks.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Case Number                  Date    Year        Type         Country  \\\n",
      "0    2017.06.11             11-Jun-17  2017.0  Unprovoked       AUSTRALIA   \n",
      "1  2017.06.10.b             10-Jun-17  2017.0  Unprovoked       AUSTRALIA   \n",
      "2  2017.06.10.a             10-Jun-17  2017.0  Unprovoked             USA   \n",
      "3  2017.06.07.R  Reported 07-Jun-2017  2017.0  Unprovoked  UNITED KINGDOM   \n",
      "4    2017.06.04             04-Jun-17  2017.0  Unprovoked             USA   \n",
      "\n",
      "                Area                                         Location  \\\n",
      "0  Western Australia                         Point Casuarina, Bunbury   \n",
      "1           Victoria                    Flinders, Mornington Penisula   \n",
      "2            Florida                      Ponce Inlet, Volusia County   \n",
      "3        South Devon                                    Bantham Beach   \n",
      "4            Florida  Middle Sambo Reef off Boca Chica, Monroe County   \n",
      "\n",
      "        Activity            Name Sex   ... Fatal   Time  \\\n",
      "0  Body boarding       Paul Goff    M  ...     N  08h30   \n",
      "1        Surfing          female    F  ...     N  15h45   \n",
      "2        Surfing     Bryan Brock    M  ...     N  10h00   \n",
      "3       Surfing     Rich Thomson    M  ...     N    NaN   \n",
      "4   Spearfishing  Parker Simpson    M  ...     N    NaN   \n",
      "\n",
      "                            Species                  Investigator or Source  \\\n",
      "0                   White shark, 4 m                    WA Today, 6/11/2017   \n",
      "1                       7 gill shark                                    NaN   \n",
      "2                                NaN  Daytona Beach News-Journal, 6/10/2017   \n",
      "3  3m shark, probably a smooth hound                         C. Moore, GSAF   \n",
      "4                           8' shark                    Nine News, 6/7/2017   \n",
      "\n",
      "                         pdf  \\\n",
      "0        2017.06.11-Goff.pdf   \n",
      "1  2017.06.10.b-Flinders.pdf   \n",
      "2     2017.06.10.a-Brock.pdf   \n",
      "3   2017.06.07.R-Thomson.pdf   \n",
      "4     2017.06.04-Simpson.pdf   \n",
      "\n",
      "                                        href formula  \\\n",
      "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "\n",
      "                                                href Case Number.1  \\\n",
      "0  http://sharkattackfile.net/spreadsheets/pdf_di...    2017.06.11   \n",
      "1  http://sharkattackfile.net/spreadsheets/pdf_di...  2017.06.10.b   \n",
      "2  http://sharkattackfile.net/spreadsheets/pdf_di...  2017.06.10.a   \n",
      "3  http://sharkattackfile.net/spreadsheets/pdf_di...  2017.06.07.R   \n",
      "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2017.06.04   \n",
      "\n",
      "  Case Number.2 original order  \n",
      "0    2017.06.11           6095  \n",
      "1  2017.06.10.b           6094  \n",
      "2  2017.06.10.a           6093  \n",
      "3  2017.06.07.R           6092  \n",
      "4    2017.06.04           6091  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "df_shark = pd.read_csv(dir+'\\datasets\\\\attacks.csv', encoding = \"ISO-8859-1\")\n",
    "print(df_shark.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) What location globally has the highest number of shark attacks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "Highest number of shark attacks (162) happened in \"New Smyrna Beach, Volusia County\"\n"
     ]
    }
   ],
   "source": [
    "df_shark_loc = df_shark.groupby(['Location'])['Case Number'].count()\n",
    "print(df_shark_loc[df_shark_loc==df_shark_loc.max()][0])\n",
    "print('Highest number of shark attacks ({1}) happened in \"{0}\"'.format(df_shark_loc[df_shark_loc==df_shark_loc.max()].index[0],df_shark_loc[df_shark_loc==df_shark_loc.max()][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Read the shark attack dataset into a Pandas Dataframe. Determine the six countries that have experienced the highest number of shark attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "USA             2160\n",
      "AUSTRALIA       1303\n",
      "SOUTH AFRICA     571\n",
      "Name: Case Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_shark_country = df_shark.groupby(['Country'])['Case Number'].count()\n",
    "#df.nlargest(3, 'a')\n",
    "print(df_shark_country.nlargest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Modify your code to print out the six countries that have experienced the highest number of fatal shark attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "AUSTRALIA       342\n",
      "USA             250\n",
      "SOUTH AFRICA    137\n",
      "Name: Case Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_shark_country_fatal = df_shark[df_shark.Fatal=='Y'].groupby(['Country'])['Case Number'].count()\n",
    "print(df_shark_country_fatal.nlargest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Based on the data in the Activity column are you more likely to be attacked by a shark if you are “Surfing” or “Scuba Diving”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shark Attacks Surfing 931 and shark attaches 74\n"
     ]
    }
   ],
   "source": [
    "df_shark_activity = df_shark.groupby(['Activity'])['Case Number'].count()\n",
    "print('Shark Attacks Surfing {0} and shark attaches {1}'.format(df_shark_activity['Surfing'],df_shark_activity['Scuba diving']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) Determine from the dataset what percentage of all recorded shark attacks were fatal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4391\n",
      "Percentage of shark attacks fatal 35.66385789114097\n"
     ]
    }
   ],
   "source": [
    "print(df_shark[df_shark.Fatal=='N']['Case Number'].count())\n",
    "print('Percentage of shark attacks fatal {0}'.\n",
    "      format((df_shark[df_shark.Fatal=='Y']['Case Number'].count()/df_shark[df_shark.Fatal=='N']['Case Number'].count())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (vi) For each individual country, print out the percentage of fatal shark attacks (number of fatal shark attacks expressed as a percentage of the total number of shark attacks). Some countries have recorded 0 fatal and non-fatal attacks. Your code should only consider countries where the number of non-fatal and fatal attacks are greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           0  Case Number\n",
      " PHILIPPINES                             1.0          NaN\n",
      " TONGA                                   2.0          1.0\n",
      "ADMIRALTY ISLANDS                        NaN          1.0\n",
      "ALGERIA                                  1.0          NaN\n",
      "AMERICAN SAMOA                           3.0          NaN\n",
      "ANDAMAN / NICOBAR ISLANDAS               1.0          NaN\n",
      "ANGOLA                                   NaN          1.0\n",
      "ANTIGUA                                  NaN          1.0\n",
      "ARGENTINA                                NaN          1.0\n",
      "ARUBA                                    1.0          NaN\n",
      "ASIA?                                    NaN          1.0\n",
      "ATLANTIC OCEAN                           7.0          8.0\n",
      "AUSTRALIA                              342.0        924.0\n",
      "AZORES                                   NaN          5.0\n",
      "BAHAMAS                                 12.0         89.0\n",
      "BAHREIN                                  1.0          NaN\n",
      "BANGLADESH                               1.0          NaN\n",
      "BARBADOS                                 4.0          2.0\n",
      "BAY OF BENGAL                            1.0          NaN\n",
      "BELIZE                                   2.0          1.0\n",
      "BERMUDA                                  3.0         12.0\n",
      "BRAZIL                                  40.0         61.0\n",
      "BRITISH ISLES                            NaN          1.0\n",
      "BRITISH NEW GUINEA                       NaN          1.0\n",
      "BRITISH VIRGIN ISLANDS                   1.0          NaN\n",
      "BRITISH WEST INDIES                      NaN          1.0\n",
      "BURMA                                    3.0          1.0\n",
      "Between PORTUGAL & INDIA                 1.0          NaN\n",
      "CANADA                                   2.0          8.0\n",
      "CAPE VERDE                               2.0          1.0\n",
      "...                                      ...          ...\n",
      "ST HELENA, British overseas territory    NaN          1.0\n",
      "ST. MAARTIN                              NaN          1.0\n",
      "ST. MARTIN                               NaN          1.0\n",
      "SUDAN                                    1.0          3.0\n",
      "SUDAN?                                   NaN          1.0\n",
      "SYRIA                                    1.0          NaN\n",
      "Seychelles                               1.0          NaN\n",
      "Sierra Leone                             1.0          NaN\n",
      "TAIWAN                                   3.0          6.0\n",
      "TANZANIA                                 6.0          2.0\n",
      "TASMAN SEA                               NaN          1.0\n",
      "THAILAND                                 3.0          2.0\n",
      "THE BALKANS                              1.0          NaN\n",
      "TONGA                                    4.0         11.0\n",
      "TRINIDAD & TOBAGO                        1.0          2.0\n",
      "TUNISIA                                  1.0          1.0\n",
      "TURKEY                                   3.0          8.0\n",
      "TURKS & CAICOS                           2.0          3.0\n",
      "TUVALU                                   1.0          NaN\n",
      "UNITED ARAB EMIRATES                     NaN          2.0\n",
      "UNITED ARAB EMIRATES (UAE)               NaN          2.0\n",
      "UNITED KINGDOM                           2.0          9.0\n",
      "URUGUAY                                  1.0          2.0\n",
      "USA                                    250.0       1879.0\n",
      "VANUATU                                  9.0          4.0\n",
      "VENEZUELA                                5.0          6.0\n",
      "VIETNAM                                  1.0         13.0\n",
      "WESTERN SAMOA                            1.0          NaN\n",
      "YEMEN                                    2.0          NaN\n",
      "YEMEN                                    5.0          1.0\n",
      "\n",
      "[203 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_shark_country_fatal = df_shark[df_shark.Fatal=='Y'].groupby(['Country'])['Case Number'].count().rename(columns={'count':'fatalcount'})\n",
    "df_shark_country_nonfatal = df_shark[df_shark.Fatal=='N'].groupby(['Country'])['Case Number'].count()\n",
    "final_frame=pd.concat([df_shark_country_fatal , df_shark_country_nonfatal],axis=1)\n",
    "print(final_frame)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
